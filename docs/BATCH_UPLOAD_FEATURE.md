# Batch Content Upload Feature - Implementation Summary

**Date**: October 4, 2025  
**Feature**: CSV Batch Upload for Content/Discoveries  
**Service**: Crawler Service  
**Status**: âœ… **COMPLETE**

---

## ðŸ“‹ Overview

Admin users can now bulk upload content via CSV files. The system automatically:
1. âœ… Parses CSV file (up to 2000 rows, 10MB max)
2. âœ… Validates URLs and metadata
3. âœ… Crawls and scrapes each URL for metadata
4. âœ… Extracts missing metadata (title, description, image, etc.)
5. âœ… Inserts content into database (status: 'pending' for moderation)
6. âœ… Returns detailed success/failure report

---

## ðŸ—ï¸ Architecture

### Backend (Crawler Service)

**New Files:**
- `apis/crawler-service/src/routes/batch.ts` - Batch upload routes

**Modified Files:**
- `apis/crawler-service/package.json` - Added dependencies
- `apis/crawler-service/src/server.ts` - Registered multipart & batch routes

**Dependencies Added:**
- `@fastify/multipart@^8.3.0` - File upload handling
- `csv-parse@^5.5.6` - CSV parsing

### Frontend (Portal)

**New Files:**
- `ui/portal/components/batch-upload.tsx` - Upload UI component

**Modified Files:**
- `ui/portal/components/admin-dashboard.tsx` - Integrated upload component
- `ui/portal/lib/api-client.ts` - Added `CrawlerAPI.batchUpload()`

---

## ðŸ“¡ API Endpoints

### POST `/api/admin/batch-upload`

**Authentication**: Admin only (via `requireAdmin` middleware)  
**Content-Type**: `multipart/form-data`  
**Max File Size**: 10MB  
**Max Rows**: 1000 per upload

**Request:**
```http
POST /api/admin/batch-upload
Authorization: Bearer {admin-clerk-jwt}
Content-Type: multipart/form-data

--boundary
Content-Disposition: form-data; name="file"; filename="content.csv"
Content-Type: text/csv

{CSV content}
--boundary--
```

**Response:**
```json
{
  "success": true,
  "summary": {
    "totalRows": 100,
    "processed": 100,
    "succeeded": 95,
    "failed": 5
  },
  "results": [
    {
      "row": 1,
      "url": "https://example.com/article1",
      "success": true,
      "contentId": "uuid-1234"
    },
    {
      "row": 42,
      "url": "https://bad-url.com",
      "success": false,
      "error": "URL already exists in database"
    }
  ]
}
```

### GET `/api/admin/batch-history`

**Authentication**: Admin only  
**Returns**: Recent batch uploads (last 100 items)

**Response:**
```json
{
  "success": true,
  "items": [
    {
      "id": "uuid-1",
      "url": "https://example.com/article1",
      "title": "Article Title",
      "created_at": "2025-10-04T10:00:00Z",
      "status": "pending"
    }
  ]
}
```

---

## ðŸ“„ CSV Format

### Required Columns
- `url` - Full HTTPS URL (required)

### Optional Columns
- `title` - Content title
- `description` - Content description
- `topics` - Comma or semicolon-separated topics (e.g., "tech,ai,ml")
- `author` - Author name
- `published_date` - ISO date or human-readable date
- `image_url` - Full HTTPS URL to featured image

### Example CSV

```csv
url,title,description,topics,author,published_date,image_url
https://example.com/article1,Understanding ML,A guide to ML,"tech,ai",Dr. Jane,2025-10-01,https://example.com/img1.jpg
https://example.com/article2,Climate Solutions,Combat climate change,"science",John Doe,2025-10-02,https://example.com/img2.jpg
```

**Sample File**: `stumbleable/sample-batch-upload.csv`

---

## ðŸ”„ Processing Flow

```
1. User uploads CSV â†’ Admin Dashboard
   â†“
2. Frontend validates file (size, type)
   â†“
3. POST /api/admin/batch-upload (Crawler Service)
   â†“
4. Parse CSV with csv-parse
   â†“
5. For each row:
   â”œâ”€ Validate URL format
   â”œâ”€ Check if URL already exists
   â”œâ”€ Fetch and scrape URL for metadata
   â”œâ”€ Merge CSV data with scraped data (CSV takes precedence)
   â”œâ”€ Extract domain
   â””â”€ Insert into content table (status: 'pending')
   â†“
6. Return summary with successes/failures
   â†“
7. Frontend displays results
   â”œâ”€ Show statistics (total, succeeded, failed)
   â”œâ”€ List failed rows with errors
   â””â”€ Allow downloading error report CSV
```

---

## ðŸŽ¯ Data Merging Strategy

**Priority**: CSV data > Scraped data > Defaults

```typescript
// Example merge logic
const finalTitle = 
    row.title ||              // 1. CSV provided title
    processedContent.title || // 2. Scraped from page
    new URL(url).hostname;    // 3. Default to domain

const finalTopics = 
    csvTopics.length > 0 ? csvTopics :  // CSV topics
    processedContent.topics || [];      // Scraped topics
```

---

## ðŸ” Security

### Admin-Only Access
- âœ… `requireAdmin` middleware checks user role
- âœ… Verifies Clerk JWT authentication
- âœ… Queries database for user role = 'admin'

### File Validation
- âœ… File type: Only `.csv` allowed
- âœ… File size: Max 10MB
- âœ… Row count: Max 2000 rows per upload
- âœ… URL validation: Must be valid HTTPS URLs

### Rate Limiting
- âœ… 500ms delay between URL processing
- âœ… 10-second timeout per URL fetch
- âœ… Graceful error handling (continue on failures)

### Data Safety
- âœ… All content starts with `status: 'pending'`
- âœ… Requires moderation approval before going live
- âœ… Duplicate URL check (prevents re-adding existing content)
- âœ… Transaction-safe inserts

---

## ðŸŽ¨ Frontend Features

### Upload Component (`BatchUpload`)

**Location**: `/admin` dashboard (admin-only)

**Features**:
- âœ… Drag-and-drop CSV file input
- âœ… File validation (type, size)
- âœ… Upload progress indicator
- âœ… Real-time status updates
- âœ… Success/failure statistics
- âœ… Detailed error table
- âœ… Download error report CSV
- âœ… Format instructions with example

**UI Elements**:
- Instructions card with required/optional columns
- Example CSV format preview
- File upload input with validation
- Upload button with loading state
- Results summary (stats cards)
- Failed rows table with links
- Download error report button

---

## ðŸ“Š Database Schema

### Content Table

**New Records Created**:
```sql
INSERT INTO content (
    url,
    title,
    description,
    domain,
    topics,
    image_url,
    author,
    published_date,
    status,            -- 'pending' (awaits moderation)
    metadata           -- JSON with source: 'batch_upload'
) VALUES (...);
```

**Metadata JSON**:
```json
{
  "source": "batch_upload",
  "csv_provided": {
    "title": true,
    "description": false,
    "topics": true,
    "author": false,
    "published_date": true,
    "image_url": true
  },
  "processed_at": "2025-10-04T10:00:00Z"
}
```

---

## ðŸ§ª Testing

### Manual Testing

1. **Start services**:
   ```powershell
   npm run dev
   ```

2. **Navigate to admin dashboard**:
   - URL: http://localhost:3000/admin
   - Must be logged in as admin user

3. **Upload CSV**:
   - Use `sample-batch-upload.csv` as test file
   - Click "Upload" button
   - Wait for processing (should take ~1-2 minutes for 3 rows)
   - Verify results displayed

4. **Check database**:
   ```sql
   SELECT * FROM content 
   WHERE metadata->>'source' = 'batch_upload'
   ORDER BY created_at DESC 
   LIMIT 10;
   ```

### Test Cases

- âœ… Valid CSV with all columns
- âœ… CSV with only required column (URL)
- âœ… Duplicate URLs (should fail gracefully)
- âœ… Invalid URLs (should fail with error message)
- âœ… Large file (test 10MB limit)
- âœ… Many rows (test 1000 row limit)
- âœ… Non-CSV file (should reject)
- âœ… Empty CSV (should show error)
- âœ… Malformed CSV (should show parse error)

---

## ðŸ“ˆ Performance

### Processing Speed
- **Rate limit**: 500ms delay between URLs
- **Expected**: ~2 URLs per second
- **2000 rows**: ~16-20 minutes total

### Optimization Opportunities
- âš¡ Parallel processing (process 5-10 URLs concurrently)
- âš¡ Queue system (background jobs with Bull/Redis)
- âš¡ Batch database inserts (100 rows at a time)
- âš¡ Caching for duplicate domain checks

---

## ðŸš€ Future Enhancements

### Short Term
- [ ] Add batch upload history table (track uploads over time)
- [ ] Show real-time progress during upload
- [ ] Email notification when batch completes
- [ ] Retry failed rows

### Medium Term
- [ ] Schedule recurring batch uploads (cron jobs)
- [ ] Import from Google Sheets URL
- [ ] Support for other formats (JSON, XML)
- [ ] Batch edit existing content

### Long Term
- [ ] AI-powered topic extraction
- [ ] Automatic image generation (if missing)
- [ ] Content quality scoring
- [ ] Duplicate detection (similar URLs/titles)

---

## ðŸ“ Usage Instructions

### For Admins

1. **Access Admin Dashboard**
   - Navigate to `/admin`
   - Ensure you have admin role

2. **Prepare CSV File**
   - Use the sample file as a template
   - Required: `url` column with HTTPS URLs
   - Optional: title, description, topics, author, etc.
   - Max 2000 rows, 10MB file size

3. **Upload**
   - Click "Select CSV File" button
   - Choose your CSV file
   - Click "Upload" button
   - Wait for processing (progress shown)

4. **Review Results**
   - Check summary statistics
   - Review failed rows (if any)
   - Download error report for failures
   - Fix errors and re-upload failed rows

5. **Moderation**
   - All uploaded content goes to moderation queue
   - Review at `/admin/moderation`
   - Approve/reject before going live

---

## ðŸ› ï¸ Troubleshooting

### "File too large" Error
- **Cause**: CSV file > 10MB
- **Solution**: Split into smaller files (max 2000 rows each)

### "Too many rows" Error
- **Cause**: CSV has > 2000 rows
- **Solution**: Split into multiple batches

### "Invalid CSV format" Error
- **Cause**: Malformed CSV (quotes, commas, encoding)
- **Solution**: Re-save as UTF-8 CSV, check for special characters

### "URL already exists" Errors
- **Cause**: URL is already in database
- **Solution**: Check existing content, remove duplicates from CSV

### Slow Processing
- **Cause**: Many URLs to scrape (500ms delay per URL)
- **Expected**: ~2 URLs/second = ~16 minutes for 2000 rows
- **Solution**: Use smaller batches for faster feedback

### Scraping Failures
- **Cause**: Website blocking, timeout, invalid HTML
- **Solution**: CSV data still used, scraping optional

---

## âœ… Checklist

### Backend
- [x] Add CSV parsing library
- [x] Add multipart file upload support
- [x] Create batch upload endpoint
- [x] Implement CSV validation
- [x] Implement URL scraping/crawling
- [x] Implement metadata extraction
- [x] Implement database insertion
- [x] Add admin authentication
- [x] Add error handling
- [x] Register routes in server

### Frontend
- [x] Create upload component
- [x] Add file validation
- [x] Add progress indicator
- [x] Display results summary
- [x] Show failed rows
- [x] Download error report
- [x] Add to admin dashboard
- [x] Update API client

### Documentation
- [x] API endpoint documentation
- [x] CSV format specification
- [x] Usage instructions
- [x] Security considerations
- [x] Testing procedures
- [x] Sample CSV file

---

## ðŸ“š Related Files

### Backend
- `apis/crawler-service/src/routes/batch.ts`
- `apis/crawler-service/src/server.ts`
- `apis/crawler-service/package.json`
- `apis/crawler-service/src/middleware/auth.ts`

### Frontend
- `ui/portal/components/batch-upload.tsx`
- `ui/portal/components/admin-dashboard.tsx`
- `ui/portal/lib/api-client.ts`

### Documentation
- `sample-batch-upload.csv`
- `docs/BATCH_UPLOAD_FEATURE.md` (this file)

---

**Implementation Complete**: October 4, 2025  
**Next Steps**: Test with real data, gather admin feedback, optimize performance
