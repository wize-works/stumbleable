# Discovery Service Optimization - Friday Launch Ready! ðŸš€

**Date**: October 3, 2025  
**Status**: âœ… Ready to deploy

## Problem

Discovery service was experiencing:
- Health probe timeouts (service restarting every ~10 minutes)
- Slow startup times
- 2-10 second response times for `/api/next` requests
- "Slow request detected" warnings (>1 second for health checks)

## Root Cause Analysis

The discovery service was doing **way too much**:
1. Core discovery (âœ… should do)
2. Content management routes
3. Metadata enhancement (CPU/memory intensive)
4. Content submission handling
5. Moderation workflows
6. Report management
7. Trending calculator (heavy background job)

All of these competing for limited resources (256MB memory, 250m CPU).

## Changes Made

### 1. Discovery Service - Stripped Down to Core âš¡

**Removed:**
- `enhance` routes â†’ Should be in crawler-service
- `submit` routes â†’ Should be in crawler-service  
- `moderation` routes â†’ Already exists in moderation-service
- `reports` routes â†’ Should be in moderation-service
- `trending-calculator` â†’ **Moved to crawler-service**

**Kept (Fast Read-Only Operations):**
- `/api/next` - Get next discovery
- `/api/trending` - Get trending content
- `/api/similar/:id` - Get similar content
- `/api/content/:id` - Get content by ID (used by /saved page)

### 2. Crawler Service - Enhanced with Background Jobs

**Added:**
- Trending calculator (runs every 15 minutes)
- Ready for content enhancement routes (future)
- Ready for submission handling (future)

### 3. Kubernetes Configuration Updates

**`k8s/base/discovery-service.yaml` changes:**
```yaml
resources:
  requests:
    memory: "256Mi"  # Was: 128Mi
    cpu: "200m"      # Was: 100m
  limits:
    memory: "512Mi"  # Was: 256Mi
    cpu: "500m"      # Was: 250m

livenessProbe:
  timeoutSeconds: 10        # Was: 5
  initialDelaySeconds: 30    # Was: 20
  failureThreshold: 5        # Was: 3

readinessProbe:
  timeoutSeconds: 8          # Was: 3
  initialDelaySeconds: 15    # Was: 10
```

### 4. Health Endpoint Optimization

```typescript
// BEFORE: Health endpoint could call external services
fastify.get('/health', async () => {
    return {
        status: 'healthy',
        rateLimit: { /* complex config */ }
    };
});

// AFTER: Ultra-fast, dependency-free
fastify.get('/health', async (request, reply) => {
    reply.code(200).send({
        status: 'healthy',
        service: 'discovery-service',
        timestamp: new Date().toISOString(),
        version: '1.0.0'
    });
});
```

## Performance Improvements Expected

### Before
- Health checks: 8ms â†’ 1195ms (timeout)
- `/api/next`: 2000-10000ms
- Restart every ~10 minutes (probe failures)
- Trending calculator competing with discovery requests

### After
- Health checks: <50ms consistently
- `/api/next`: <500ms (with our parallelized DB queries)
- No more restarts
- Background jobs in separate service

## Architecture Benefits

âœ… **Single Responsibility** - Discovery service only does discovery  
âœ… **Better Resource Allocation** - Heavy operations don't impact discovery  
âœ… **Independent Scaling** - Scale discovery separately from content management  
âœ… **Fault Isolation** - Background job issues don't break discovery  
âœ… **Cleaner Codebase** - Each service has clear purpose

## Deployment Instructions

### Option 1: Via GitHub Actions (Recommended)
```bash
git add .
git commit -m "âš¡ Optimize discovery service - strip to core, move trending to crawler"
git push origin main
```

The workflow will:
1. Build updated images for discovery-service and crawler-service
2. Deploy with new K8s configs
3. Rolling update (zero downtime)

### Option 2: Manual (if needed)
```bash
# Build and push images
cd apis/discovery-service
docker build -t brandonkorouscontainers.azurecr.io/stumbleable-discovery:latest .
docker push brandonkorouscontainers.azurecr.io/stumbleable-discovery:latest

cd ../crawler-service
docker build -t brandonkorouscontainers.azurecr.io/stumbleable-crawler:latest .
docker push brandonkorouscontainers.azurecr.io/stumbleable-crawler:latest

# Apply K8s configs
kubectl apply -f k8s/base/discovery-service.yaml
kubectl apply -f k8s/base/crawler-service.yaml

# Watch the rollout
kubectl rollout status deployment/discovery-service -n stumbleable
kubectl rollout status deployment/crawler-service -n stumbleable
```

## Verification Steps

After deployment:

```bash
# 1. Check pod health
kubectl get pods -n stumbleable

# 2. Watch discovery service logs
kubectl logs -f deployment/discovery-service -n stumbleable

# 3. Check health endpoint
curl https://stumbleable.com/api/discovery/health

# 4. Test discovery endpoint
# (from frontend - click Stumble button)

# 5. Verify trending calculator in crawler
kubectl logs -f deployment/crawler-service -n stumbleable | grep "Trending"
```

Expected log output:
```
ðŸš€ Discovery Service running on http://0.0.0.0:8080
ðŸ“Š Health check: http://0.0.0.0:8080/health
ðŸ” Next discovery: POST http://0.0.0.0:8080/api/next
ðŸ“ˆ Trending: GET http://0.0.0.0:8080/api/trending
ðŸ”„ Similar: GET http://0.0.0.0:8080/api/similar/:id
âš¡ Focused on fast discovery - content management moved to crawler-service
```

## Rollback Plan (if needed)

```bash
# Rollback discovery service
kubectl rollout undo deployment/discovery-service -n stumbleable

# Rollback crawler service
kubectl rollout undo deployment/crawler-service -n stumbleable
```

## Future Improvements

1. **Add Redis caching** for discovery results (1-5 minute TTL)
2. **Move remaining heavy routes**:
   - `enhance` â†’ crawler-service
   - `submit` â†’ crawler-service
   - `reports` â†’ moderation-service
3. **Add connection pooling** for Supabase (if not already present)
4. **Consider read replicas** for discovery queries

## Files Changed

- âœï¸ `apis/discovery-service/src/server.ts` - Removed heavy routes
- âœï¸ `apis/crawler-service/src/server.ts` - Added trending calculator
- âž• `apis/crawler-service/src/lib/trending-calculator.ts` - Copied from discovery
- âœï¸ `k8s/base/discovery-service.yaml` - Increased resources & probe timeouts
- âž• `DISCOVERY_SERVICE_OPTIMIZATION.md` - This document

## Success Criteria

- âœ… No pod restarts for 1 hour
- âœ… Health checks < 100ms
- âœ… `/api/next` responses < 500ms  
- âœ… Trending calculator runs successfully every 15 minutes
- âœ… No 500 errors in discovery service
- âœ… Discovery service stays under 400MB memory

---

**Ready to launch! ðŸŽ‰**

Perfect timing for a Friday afternoon deployment. The changes are focused, tested locally, and follow your existing architecture patterns.
